[global]
# OPTIONS THAT APPLY TO ALL MODELS

# Use ICNR Kernel Initializer for upscaling.
# This can help reduce the 'checkerboard effect' when upscaling the image.
# Choose from: True, False
# [Default: False]
icnr_init = False

# Use subpixel upscaling rather than pixel shuffler.
# Might increase speed at cost of VRAM
# Choose from: True, False
# [Default: False]
subpixel_upscaling = False

# Use reflect padding rather than zero padding.
# Choose from: True, False
# [Default: False]
reflect_padding = False

# If using a mask, Use DSSIM loss for Mask training rather than Mean Absolute Error
# May increase overall quality.
# Choose from: True, False
# [Default: True]
dssim_mask_loss = True

# If using a mask, Use Penalized loss for Mask training. Can stack with DSSIM.
# May increase overall quality.
# Choose from: True, False
# [Default: True]
penalized_mask_loss = True

[model.dfaker]
# DFAKER MODEL (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: dfaker]
mask_type = dfaker

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 100.0]
coverage = 100.0

[model.dfl_h128]
# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: dfl_full]
mask_type = dfl_full

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 62.5]
coverage = 62.5

[model.iae]
# INTERMEDIATE AUTO ENCODER. BASED ON ORIGINAL MODEL, USES INTERMEDIATE LAYERS TO TRY TO BETTER GET DETAILS

# Use DSSIM for Loss rather than Mean Absolute Error
# May increase overall quality.
# Choose from: True, False
# [Default: False]
dssim_loss = False

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: none]
mask_type = none

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 62.5]
coverage = 62.5

[model.original]
# ORIGINAL FACESWAP MODEL

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

# Use DSSIM for Loss rather than Mean Absolute Error
# May increase overall quality.
# Choose from: True, False
# [Default: False]
dssim_loss = False

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: none]
mask_type = none

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 62.5]
coverage = 62.5

[model.unbalanced]
# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.
# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other. NB: lowmem will override cutom nodes and complexity settings.
# Choose from: True, False
# [Default: False]
lowmem = False

# Use DSSIM for Loss rather than Mean Absolute Error
# May increase overall quality.
# Choose from: True, False
# [Default: False]
dssim_loss = False

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: none]
mask_type = none

# Number of nodes for decoder. Don't change this unless you know what you are doing!
# Select an integer between 512 and 4096
# [Default: 1024]
nodes = 1024

# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160
# Select an integer between 64 and 1024
# [Default: 128]
complexity_encoder = 128

# Decoder A Complexity.
# Select an integer between 64 and 1024
# [Default: 384]
complexity_decoder_a = 384

# Decoder B Complexity.
# Select an integer between 64 and 1024
# [Default: 512]
complexity_decoder_b = 512

# Resolution (in pixels) of the image to train on.
# BE AWARE Larger resolution will dramatically increaseVRAM requirements.
# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).
# NB: Your faceset must be at least 1.6x larger than your required input size.
#     (e.g. 160 is the maximum input size for a 256x256 faceset)
# Select an integer between 64 and 512
# [Default: 128]
input_size = 128

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 62.5]
coverage = 62.5

[model.villain]
# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.
# EXTREMELY VRAM HEAVY. FULL MODEL REQUIRES 9GB+ FOR BATCHSIZE 16

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

# Use DSSIM for Loss rather than Mean Absolute Error
# May increase overall quality.
# Choose from: True, False
# [Default: False]
dssim_loss = False

# The mask to be used for training. Select none to not use a mask
# Choose from: ['none', 'dfaker', 'dfl_full']
# [Default: none]
mask_type = none

# How much of the extracted image to train on. Generally the model is optimized
# to the default value. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 62.5]
coverage = 62.5

