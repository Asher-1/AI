## 1.2 TensorFlow的设计目标

TensorFlow作为Google公司上一代深度学习平台DistBelief的继任者，其首要设计目标是满足公司内部的图像、语音、语义等感知和预测类应用的需求。这些应用的数据规模和模型复杂度日益增长，对提供计算能力的软硬件平台的功能和性能不断提出更高的要求。与此同时，新一波人工智能浪潮的兴起也促使在开源软件商业生态建设方面经验丰厚的Google公司敏锐嗅探信息化、智能化产业发展的新动向，并广泛吸收工业界、学术界和开源社区的前沿理念与最佳实践，将这些思想纳入TensorFlow的设计。种种迹象表明，TensorFlow的设计目标并非局限一套深度学习库，Google寄希望其成为一套面向多种应用场景和编程范式、支持异构计算平台、具备优异性能与可伸缩性的通用人工智能引擎。本节从中选取几个侧面，对TensorFlow的设计目标进行介绍和分析。

### 1.2.1 灵活通用的深度学习库

近几年来，随着海量数据的涌现、硬件计算能力的提升以及神经网络模型和机器学习算法的改进，深度学习技术得到了快速的发展，已经成为学术界和工业界共同关注的热点。一方面，深度学习模型和算法的理论研究还未完全成熟，其发展空间巨大，吸引了大量科研人员参与其中。这几年深度学习席卷了各大顶级学术会议，模型设计、方法优化和应用创新的迭代更新速度极快。另一方面，深度学习在某些领域已经可以落地，比如人脸识别模型成功应用于安防系统，语音识别模型成功用于智能终端。除此之外，工业界正在积极探索深度学习更多潜在的商业应用场景。在人工智能的热潮中，很多开源的深度学习库应运而生，这对加速相关研究和工程化效率起到了非常重要的作用。为了应对上述研究与应用领域的诸多使用场景，深度学习库必须注重设计的灵活性与功能的通用性，才能在风起云涌的人工智能生态系统中得以立足。TensorFlow作为当前主流的深度学习库之一，其设计具有很高的灵活性和通用性，主要体现在以下几个方面。

在算子定义方面，相比于其他深度学习库，TensorFlow提供的算子粒度更细、数量更多，能够支撑上层灵活多变的模型和算法。用户可以使用这些算子自由、灵活地开发各种深度学习模型。此外，很多传统的机器学习模型也可以基于TensorFlow实现，如支持向量机、决策树和随机森林等。TensorFlow亦支持深度学习和传统机器学习混合的模型，从而使得数据流水线式的应用创新成为可能。TensorFlow对新算子的支持也足够灵活，允许用户通过组合已有的细粒度算子来构造新的算子，以快速实现算法原型、验证一些新的想法。用户也可以使用C++语言和CUDA等底层函数库实现新的算子并在运行时动态加载使用，以便满足专用算法需求并保证计算性能。

在编程范式方面，TensorFlow支持声明式编程，将模型的定义和执行解耦。模型以数据流图结构表示，经过编译和优化之后才会真正执行。以数据流图抽象为核心的设计在保证模型执行效率的同时，使得用户编程更加灵活。例如，在模型定义阶段，用户可以通过添加控制依赖边来指定算子的执行顺序，可以通过添加自定义变量自如地管理数据流图的输入输出，还可以通过队列控制多设备之间的数据传输和子图执行时序。在数据流图的运行态，用户可以指定数据流图中待执行的子图，从而避免不必要的算子计算开销。除了模型之外，数据读取、数据预处理等其他操作都可以被添加到数据流图中，用户可以通过编辑数据流图实现对具体应用的端到端灵活控制。

在运行时框架方面，TensorFlow在具备隐式并行计算能力的同时，也提供了细粒度的显式控制接口，允许灵活地控制模型在多节点异构设备上的分布式执行方式。用户在编写深度学习模型时，可以自由地将模型中的每个算子绑定在任意的计算设备上。TensorFlow运行时框架负责将模型对应的数据流图按照设备进行切分，并自动插入必要的通信操作，对用户屏蔽了底层复杂的数据传输与时序同步机制。用户可以结合具体模型和应用的特点，通过手工指定或者以强化学习等方式找出模型在多节点异构硬件上的最优布局与执行方式（如数据并行、模型并行等），这使得模型的训练和推理都有更多的优化空间。

在多语言支持方面，TensorFlow提供Python、C、C++、Java、Go等主流语言的编程接口。虽然Python是当前深度学习和人工智能领域使用最为广泛的编程语言，但是其他语言也有各自的语法优势、适用场景以及拥趸用户，它们能够满足科研、商用等不同应用领域及服务器、终端等不同目标设备的开发需求。另外，社区开源贡献者在TensorFlow C API基础上扩展开发了Node.js、Julia、R等其他语言的编程接口，这在体现TensorFlow内核设计灵活性的同时，也进一步扩大了其作为通用深度学习库的场景覆盖范围。

总而言之，TensorFlow通过丰富的算子、灵活的编程范式、自由的运行时框架以及多语言API支持，对用户展现了高度的灵活性和通用性。TensorFlow在其设计之初就被定义为灵活通用的深度学习平台。Google公司将它开源的目的正是要以其作为基石，构筑深度学习乃至整个人工智能的生态圈。

### 1.2.2 端云结合的人工智能引擎

“端云结合”是当今信息化、智能化技术发展的普遍趋势。一方面，随着信息技术在生产、生活中各个应用领域愈发深入的集成，对海量数据的高效处理成为IT服务商与决策部门的迫切需求。在传统数据中心基础上发展起来的云计算和大数据技术以集约化的资源管理、动态弹性的资源供给为持续膨胀的应用提供了高水平、可伸缩的计算能力，同时降低了服务提供者的准入门槛。这就要求传统服务器端软件必须适应云化部署场景，以水平扩展（scale-out）、无状态、微服务等方式构建高内聚、低耦合的系统架构。另一方面，随着以智能手机为代表的移动终端技术的高速普及，以及物联网、机器人等智能化技术在传统行业的不断渗透，用户对于数据私密性、安全性的重视程度逐渐增强，应用场景对服务实时性与可用性的要求也更加严格。在这一背景下，计算能力的边缘化成为与云化并驾齐驱的演进方向。这就要求提供服务的软件能够适应体系结构多样、计算资源有限、功耗受到制约的终端硬件环境，并具备一定的自治与协同工作能力。

在人工智能领域野心勃勃的Google公司自然准确地把握并积极地引领着这一趋势。在云侧，Google既是一家公有云提供商，需要通过具有核心竞争力的PaaS层产品吸引企业级用户和二次开发者；又是一家业务高度依赖于智能算法与海量数据的创新型公司，需要开发高效、灵活的基础平台软件满足自身业务快速发展的需要。在端侧，Google公司不但需要为日新月异的应用开发提供强有力的智能化支撑能力，从而使其Android生态系统得以抗衡强大的竞争对手；而且需要借助实时且可靠的智能计算引擎进军物联网、可穿戴设备、增强实现、无人驾驶等新兴领域，以寻求“后移动互联网时代”的新增长点。TensorFlow等平台层软件的设计因此也兼顾了云侧与端侧的需求。

TensorFlow对云计算场景的支持是其竞争力的基础，主要体现在以下方面：（1）提供多种标准化的安装包、构建脚本及容器化封装，支持在不同Linux发行版以及Windows Server等其他服务器操作系统上部署。既允许以二进制包方式快速安装，也允许针对特定环境定制化编译高效的目标代码，从而极大地增强了软件的适用范围和适配能力。（2）支持对接多种常见的公有云和私有云服务，如Google Cloud Storage、Amazon S3、HDFS，并为对接其他类似服务预留可扩展设计，从而能够与既有的互联网、大数据生态系统无缝交互，实现资源复用与服务组合。（3）兼容若干种常见的高性能计算与通信硬件，能够有效利用云环境的既有投资并提升应用软件对高端硬件资源的利用率。例如，支持NVIDIA和OpenCL GPU，能够充分挖掘众核设备的并行计算能力；支持RDMA网络协议，能够充分发挥InfiniBand等高速网络设备的带宽潜力。（4）灵活的运行时框架设计，既提供标准且易用的PS-worker分布式模式，也允许用户自由开发针对特定环境需求的分布式框架。即使脱离Google公司的Borg等基础设施，以TensorFlowOnSpark、MaTEx-TensorFlow为代表的第三方工具也能够利用既有的分布式平台提升TensorFlow在数据中心和超算集群中的可伸缩性。

TensorFlow在端侧场景方面也毫不逊色，其主要设计体现在：（1）推理（预测）态代码能够运行于多种主流的终端平台，包括Android、iOS，以及部署Linux操作系统的多类ARM与MIPS设备（如Raspberry Pi），从而为形态多样的终端设备集成AI认知与决策能力提供支撑。（2）通过XLA AOT（ahead-of-time）编译技术及其他软硬件解耦设计，显著地简化底层异构计算设备的对接方式，实现对神经网络芯片等新型专用端侧硬件的快速支持能力。（3）提供量化参数和低精度代数等算法层机制，适配算力、存储和功耗受限的终端，从而实现低端边缘设备的智能化。（4）提供模型与框架一体化的精简版运行时平台，具备完全的离线工作能力，有助于实现端侧计算的私密性与实时性。

可以看出，TensorFlow作为一套人工智能引擎，不但致力于增强应用系统的“大脑”，同时也在帮助其完善“末梢神经”。可以预见，未来TensorFlow等人工智能引擎会像Linux操作系统内核一样，成为在端云两侧广泛支撑各类应用、助力实现智能化社会的幕后英雄。

### 1.2.3 高性能的基础平台软件

虽然如今的互联网、大数据计算平台软件层出不穷，核心技术变幻莫测，吸睛特性轮番登场，但是性能始终都能够超脱于名目繁多的噱头，成为几乎所有用户一致认可的硬指标。在半导体器件物理极限将至的情况下，摩尔定律的有效性已经存疑，软件轻松分享硬件发展红利的时代走向末路。硬件设计者正在广泛采纳新型器件、三维电路、应用定制、众核并行等多元化思路满足应用不断增长的算力需求，这为软件开发者提供机会的同时也带来了不小的挑战。如何将软件架构和算法有效适配到硬件体系结构、充分利用硬件资源发挥其设计性能，成为所有软件开发者，特别是基础平台层软件开发者面临的重要问题。

随着深度学习技术发展而兴起的一系列开源计算库长期处于激烈竞争的态势。在竞争过程中，一快遮百丑。这里的“快”字有两层含义：深度学习库的开发者不仅需要快速响应上层需求和下层技术的变化，及时发布新版本与新特性，而且需要通过苦修内功提升深度学习库本身的性能，加快算法模型的训练与推理速度。在第一个“快”字上，TensorFlow借助Google公司强大的号召力和坚决的执行力，长期保持领先地位。在第二个“快”字上，TensorFlow曾因权衡灵活性等原因一度落后于同类软件，但如今已经迎头赶上，在主流应用场景中取得了优异的测试成绩。这归功于众多核心研究者和开源贡献者在性能方面的深耕。

TensorFlow的高性能设计首先体现在它对高端和专用硬件的深入支持。同其他主流的深度学习库一样，TensorFlow将NVIDIA GPU作为训练态的硬件加速器，同时兼顾OpenCL GPU设备。不同于简单使用CUDA Runtime API的其他平台，Google的工程师基于CUDA Driver API实现了控制粒度更细、并行性能更优的StreamExecutor异构计算库，并对cuBLAS、cuDNN等库的函数变种进行了精确的适配。在推理态，尽管Google没有开源或销售TPU，然而TensorFlow开放性的设计已经促使多家芯片厂家实现了对接，这为定制化设备上的计算性能提升提供了保障。针对高性能计算环境中常用的InfiniBand、RoCE等高速网络设备，以及NVLink等片间高速互联技术，TensorFlow引入了RDMA、NCCL等协议，较好地解决了通信延迟问题，推进了分布式计算作业的加速比提升。

其次，系统层的优化技术是TensorFlow性能提升的重要杀手锏。相比来自于学术界的算法研究团队，Google科研与工程团队深厚的系统研发背景是TensorFlow构建性能竞争力的坚实后盾。XLA这种融合了编译器设计理论的优化框架就是一例。它引入的JIT（just-in-time）编译机制能够在数据流图运行过程中实时创建二进制代码，将其中大量细粒度的操作融合为少量粗粒度的专用核函数，从而减少图中操作执行时的内存分配和上下文切换开销，极大地提升计算速度。TensorFlow诸多模块设计中也存在着细节性能优化。例如，通信模块中具有若干种旁路（bypass）设计，可以避免不必要的网络访问和内存复制开销；数据流图构建时会执行常量折叠、公共子表达式消除、内联函数展开等多种语法树优化，能够消除无意义的计算开销。这些设计体现出开发者良好的软件工程素养与精益求精追求。

最后，算法层的优化设计也是TensorFlow实现优异性能不可或缺的组成部分。为了实现高性能的目标，TensorFlow的设计采纳了自顶向下、全栈优化的思路，而算子恰恰是贯穿上下层的核心要素。在深度学习的算法模型中，每种算子的逻辑都可以采用多种算法实现。为此，TensorFlow内置了多种优化后的基础算子和模型组件。以卷积算子为例，cuDNN提供了winograd等8种算法。针对不同的输入数据大小、卷积计算超参以及内存等资源限制，TensorFlow会自动为每个卷积操作选择最快的实现算法。另外，针对递归神经网络等模型，TensorFlow也支持Fold解决方案，使得动态批处理成为可能，极大加速了这些模型的计算速度。

综上所述，性能是TensorFlow研发者重点关注的设计目标。虽然TensorFlow开源版本的性能优化起步稍晚，但是在Google团队和开源社区的共同努力下进步迅速。在这个“天下武功唯快不破”的时代，TensorFlow的高性能优势必将为其插上腾飞的翅膀，使之引领人工智能研究与应用的高速发展。

#

**Prev：**[1.1 TensorFlow简介](1.1_introduction.md)

**Next：**[1.3 TensorFlow的基本架构](1.3_architecture.md)