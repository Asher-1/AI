{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my code which compares pictures in original project lmdb and hdfd5\n",
    "# actually we train test on different pictures which not allow us to directly compare losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import caffe\n",
    "import struct\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = lmdb.open(\"/home/anatolix/iidf-data/Realtime_Pose_Estimation_LMDB\", readonly=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(key, array):\n",
    "    metadata=array[3]\n",
    "    \n",
    "    #0 dataset name\n",
    "    dataset = struct.unpack('@10s',metadata[0].tobytes()[:10])[0]\n",
    "    dataset = dataset.partition(b'\\x00')[0]\n",
    "    \n",
    "    #1 image height width\n",
    "    height, width = struct.unpack('ff',metadata[1].tobytes()[:8])\n",
    "    \n",
    "    #2 validation, numother, people_index, 4*annolist_index, write_count\n",
    "    validation, numother, people_index, annolist_index, write_count, total_count  = struct.unpack('<?BBfff',metadata[2].tobytes()[:15])\n",
    "\n",
    "    # repeated for each person\n",
    "    #3 objpos_x, objpos_y \n",
    "    #4 scale_povided\n",
    "    #4,5,6 joint self 3*16\n",
    "    \n",
    "    #print(dataset, height, width, validation, numother, people_index, annolist_index, write_count, total_count)\n",
    "\n",
    "    idx=3\n",
    "\n",
    "    objpos = []\n",
    "    joints = []\n",
    "    \n",
    "    objpos_x, objpos_y = struct.unpack('<ff',metadata[idx].tobytes()[:8])\n",
    "    idx += 1\n",
    "    objpos.append([objpos_x, objpos_y])\n",
    "    \n",
    "    scale_povided = struct.unpack('<f',metadata[idx].tobytes()[:4])[0]\n",
    "    idx += 1\n",
    "\n",
    "    #print(objpos_x, objpos_y, scale_povided)\n",
    "\n",
    "    joints_x = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "    idx += 1        \n",
    "    joints_y = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "    idx += 1        \n",
    "    joints_v = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "    idx += 1        \n",
    "    #print(joints_x, joints_y, joints_v,\"\\n\")\n",
    "    joints.append([joints_x, joints_y, joints_v])\n",
    "    \n",
    "    for i in range(numother):\n",
    "        objpos_x, objpos_y = struct.unpack('<ff',metadata[idx].tobytes()[:8])\n",
    "        idx += 1\n",
    "        #print(objpos_x, objpos_y)\n",
    "        objpos.append([objpos_x, objpos_y])\n",
    "        \n",
    "    scale_povided = struct.unpack('<'+'f'*numother,metadata[idx].tobytes()[:4*numother])\n",
    "    #print(scale_povided)\n",
    "    idx += 1        \n",
    "        \n",
    "    for i in range(numother):\n",
    "        joints_x = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "        idx += 1        \n",
    "        joints_y = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "        idx += 1        \n",
    "        joints_v = struct.unpack('f'*16,metadata[idx].tobytes()[:64])\n",
    "        idx += 1        \n",
    "        #print(joints_x, joints_y, joints_v,\"\\n\")\n",
    "        joints.append([joints_x, joints_y, joints_v])\n",
    "\n",
    "    return (dataset.decode('ascii'), validation, int(annolist_index), int(write_count), int(total_count), int(height), int(width), np.array(joints).transpose([0,2,1]).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum = []\n",
    "with open(\"hashes_lmdb.txt\",\"w\") as f:\n",
    "    with db.begin() as env:\n",
    "        cursor = env.cursor()\n",
    "        i=0\n",
    "        for key, raw_datum in cursor:\n",
    "            datum = caffe.proto.caffe_pb2.Datum()\n",
    "            datum.ParseFromString(raw_datum)\n",
    "            array = caffe.io.datum_to_array(datum)\n",
    "            dataset, validation, annolist_index, write_count, total_count, height, width, joints  = process(key, array)\n",
    "            jhash = hashlib.sha1(joints.tobytes()).hexdigest()\n",
    "            phash = hashlib.sha1(array[:3,:,:].tobytes()).hexdigest()\n",
    "            print(i, dataset,validation, annolist_index, write_count, total_count, height, width, jhash, phash, joints.tolist(), file=f)\n",
    "            i=i+1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5 = h5py.File('dataset/val_dataset.h5', \"r\")\n",
    "datum = h5['datum']\n",
    "keys = list(datum.keys())\n",
    "\n",
    "with open(\"hashes_val.txt\",\"w\") as f:\n",
    "\n",
    "    for key in keys:\n",
    "        entry = datum[key]\n",
    "        meta = json.loads(entry.attrs['meta'])\n",
    "        img = meta['img_path']\n",
    "        height = meta['img_height']\n",
    "        width = meta['img_width']\n",
    "        isValidation = meta['isValidation']\n",
    "        dataset = meta['dataset']\n",
    "        joints = np.array(meta['joints']).astype(int)\n",
    "        jhash = hashlib.sha1(joints.tobytes()).hexdigest()\n",
    "        phash = hashlib.sha1(entry.value[:3,:,:]).hexdigest()\n",
    "       \n",
    "        print(img, dataset, bool(isValidation), height,width,jhash,phash, joints.tolist(), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5 = h5py.File('dataset/train_dataset.h5', \"r\")\n",
    "datum = h5['datum']\n",
    "keys = list(datum.keys())\n",
    "\n",
    "with open(\"hashes_train.txt\",\"w\") as f:\n",
    "\n",
    "    for key in keys:\n",
    "        entry = datum[key]\n",
    "        meta = json.loads(entry.attrs['meta'])\n",
    "        img = meta['img_path']\n",
    "        height = meta['img_height']\n",
    "        width = meta['img_width']\n",
    "        isValidation = meta['isValidation']\n",
    "        dataset = meta['dataset']\n",
    "        joints = np.array(meta['joints']).astype(int)\n",
    "        jhash = hashlib.sha1(joints.tobytes()).hexdigest()\n",
    "        phash = hashlib.sha1(entry.value[:3,:,:]).hexdigest()\n",
    "        \n",
    "        print(img, dataset, bool(isValidation), height,width,jhash,phash, joints.tolist(), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:intel2018]",
   "language": "python",
   "name": "conda-env-intel2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
