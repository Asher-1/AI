{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.设置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"ptb.train\"          # 训练数据路径。\n",
    "EVAL_DATA = \"ptb.valid\"           # 验证数据路径。\n",
    "TEST_DATA = \"ptb.test\"            # 测试数据路径。\n",
    "HIDDEN_SIZE = 300                 # 隐藏层规模。\n",
    "NUM_LAYERS = 2                    # 深层循环神经网络中LSTM结构的层数。\n",
    "VOCAB_SIZE = 10000                # 词典规模。\n",
    "TRAIN_BATCH_SIZE = 20             # 训练数据batch的大小。\n",
    "TRAIN_NUM_STEP = 35               # 训练数据截断长度。\n",
    "\n",
    "EVAL_BATCH_SIZE = 1               # 测试数据batch的大小。\n",
    "EVAL_NUM_STEP = 1                 # 测试数据截断长度。\n",
    "NUM_EPOCH = 5                     # 使用训练数据的轮数。\n",
    "LSTM_KEEP_PROB = 0.9              # LSTM节点不被dropout的概率。\n",
    "EMBEDDING_KEEP_PROB = 0.9         # 词向量不被dropout的概率。\n",
    "MAX_GRAD_NORM = 5                 # 用于控制梯度膨胀的梯度大小上限。\n",
    "SHARE_EMB_AND_SOFTMAX = True      # 在Softmax层和词向量层之间共享参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过一个PTBModel类来描述模型，这样方便维护循环神经网络中的状态。\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录使用的batch大小和截断长度。\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义每一步的输入和预期输出。两者的维度都是[batch_size, num_steps]。\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络。\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(NUM_LAYERS)]     \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)            \n",
    "        \n",
    "        # 初始化最初的状态，即全零的向量。这个量只在每个epoch初始化第一个batch\n",
    "        # 时使用。\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # 定义单词的词向量矩阵。\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将输入单词转化为词向量。\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout。\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    " \n",
    "        # 定义输出列表。在这里先将不同时刻LSTM结构的输出收集起来，再一起提供给\n",
    "        # softmax层。\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output) \n",
    "        # 把输出队列展开成[batch, hidden_size*num_steps]的形状，然后再\n",
    "        # reshape成[batch*numsteps, hidden_size]的形状。\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    " \n",
    "        # Softmax层：将RNN在每个位置上的输出转化为各个单词的logits。\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        # 定义交叉熵损失函数和平均损失。\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]),\n",
    "            logits=logits)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 只在训练模型时定义反向传播操作。\n",
    "        if not is_training: return\n",
    "\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.定义数据和训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值。\n",
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    # 计算平均perplexity的辅助变量。\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state) \n",
    "    # 训练一个epoch。\n",
    "    for x, y in batches:\n",
    "        # 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的就是下一个单\n",
    "        # 词为给定单词的概率。\n",
    "        cost, state, _ = session.run(\n",
    "             [model.cost, model.final_state, train_op],\n",
    "             {model.input_data: x, model.targets: y,\n",
    "              model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        # 只有在训练时输出日志。\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (\n",
    "                  step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "\n",
    "    # 返回给定模型在给定数据上的perplexity值。\n",
    "    return step, np.exp(total_costs / iters)\n",
    "\n",
    "\n",
    "# 从文件中读取数据，并返回包含单词编号的数组。\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        # 将整个文档读进一个长字符串。\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]  # 将读取的单词编号转为整数\n",
    "    return id_list\n",
    "\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    # 计算总的batch数量。每个batch包含的单词数量是batch_size * num_step。\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\n",
    "\n",
    "    # 如9-4图所示，将数据整理成一个维度为[batch_size, num_batches * num_step]\n",
    "    # 的二维数组。\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    # 沿着第二个维度将数据切分成num_batches个batch，存入一个数组。\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "\n",
    "    # 重复上述操作，但是每个位置向右移动一位。这里得到的是RNN每一步输出所需要预测的\n",
    "    # 下一个单词。\n",
    "    label = np.array(id_list[1 : num_batches * batch_size * num_step + 1]) \n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)  \n",
    "    # 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵。\n",
    "    return list(zip(data_batches, label_batches)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.主函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 10044.873\n",
      "After 100 steps, perplexity is 2633.552\n",
      "After 200 steps, perplexity is 1497.834\n",
      "After 300 steps, perplexity is 1145.173\n",
      "After 400 steps, perplexity is 933.455\n",
      "After 500 steps, perplexity is 790.784\n",
      "After 600 steps, perplexity is 692.096\n",
      "After 700 steps, perplexity is 616.672\n",
      "After 800 steps, perplexity is 550.913\n",
      "After 900 steps, perplexity is 502.078\n",
      "After 1000 steps, perplexity is 466.776\n",
      "After 1100 steps, perplexity is 432.407\n",
      "After 1200 steps, perplexity is 405.398\n",
      "After 1300 steps, perplexity is 380.194\n",
      "Epoch: 1 Train Perplexity: 376.035\n",
      "Epoch: 1 Eval Perplexity: 191.894\n",
      "In iteration: 2\n",
      "After 1400 steps, perplexity is 191.120\n",
      "After 1500 steps, perplexity is 175.893\n",
      "After 1600 steps, perplexity is 178.309\n",
      "After 1700 steps, perplexity is 175.451\n",
      "After 1800 steps, perplexity is 170.805\n",
      "After 1900 steps, perplexity is 168.688\n",
      "After 2000 steps, perplexity is 167.135\n",
      "After 2100 steps, perplexity is 162.046\n",
      "After 2200 steps, perplexity is 158.702\n",
      "After 2300 steps, perplexity is 157.352\n",
      "After 2400 steps, perplexity is 154.732\n",
      "After 2500 steps, perplexity is 151.697\n",
      "After 2600 steps, perplexity is 147.993\n",
      "Epoch: 2 Train Perplexity: 147.339\n",
      "Epoch: 2 Eval Perplexity: 138.610\n",
      "In iteration: 3\n",
      "After 2700 steps, perplexity is 129.512\n",
      "After 2800 steps, perplexity is 114.150\n",
      "After 2900 steps, perplexity is 120.933\n",
      "After 3000 steps, perplexity is 118.669\n",
      "After 3100 steps, perplexity is 117.678\n",
      "After 3200 steps, perplexity is 117.824\n",
      "After 3300 steps, perplexity is 117.346\n",
      "After 3400 steps, perplexity is 115.280\n",
      "After 3500 steps, perplexity is 113.166\n",
      "After 3600 steps, perplexity is 112.690\n",
      "After 3700 steps, perplexity is 112.502\n",
      "After 3800 steps, perplexity is 110.425\n",
      "After 3900 steps, perplexity is 108.450\n",
      "Epoch: 3 Train Perplexity: 107.988\n",
      "Epoch: 3 Eval Perplexity: 118.488\n",
      "In iteration: 4\n",
      "After 4000 steps, perplexity is 106.441\n",
      "After 4100 steps, perplexity is 90.129\n",
      "After 4200 steps, perplexity is 95.809\n",
      "After 4300 steps, perplexity is 95.780\n",
      "After 4400 steps, perplexity is 94.855\n",
      "After 4500 steps, perplexity is 94.425\n",
      "After 4600 steps, perplexity is 94.203\n",
      "After 4700 steps, perplexity is 93.516\n",
      "After 4800 steps, perplexity is 92.027\n",
      "After 4900 steps, perplexity is 91.551\n",
      "After 5000 steps, perplexity is 91.850\n",
      "After 5100 steps, perplexity is 90.373\n",
      "After 5200 steps, perplexity is 89.374\n",
      "After 5300 steps, perplexity is 88.893\n",
      "Epoch: 4 Train Perplexity: 88.859\n",
      "Epoch: 4 Eval Perplexity: 112.674\n",
      "In iteration: 5\n",
      "After 5400 steps, perplexity is 78.771\n",
      "After 5500 steps, perplexity is 80.283\n",
      "After 5600 steps, perplexity is 83.865\n",
      "After 5700 steps, perplexity is 81.612\n",
      "After 5800 steps, perplexity is 80.392\n",
      "After 5900 steps, perplexity is 80.628\n",
      "After 6000 steps, perplexity is 80.836\n",
      "After 6100 steps, perplexity is 79.483\n",
      "After 6200 steps, perplexity is 79.144\n",
      "After 6300 steps, perplexity is 79.731\n",
      "After 6400 steps, perplexity is 79.028\n",
      "After 6500 steps, perplexity is 78.288\n",
      "After 6600 steps, perplexity is 77.334\n",
      "Epoch: 5 Train Perplexity: 77.541\n",
      "Epoch: 5 Eval Perplexity: 108.727\n",
      "Test Perplexity: 104.901\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 定义初始化函数。\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    \n",
    "    # 定义训练用的循环神经网络模型。\n",
    "    with tf.variable_scope(\"language_model\", \n",
    "                           reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "\n",
    "    # 定义测试用的循环神经网络模型。它与train_model共用参数，但是没有dropout。\n",
    "    with tf.variable_scope(\"language_model\",\n",
    "                           reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "    # 训练模型。\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_batches = make_batches(\n",
    "            read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "        eval_batches = make_batches(\n",
    "            read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "        test_batches = make_batches(\n",
    "            read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "        step = 0\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            step, train_pplx = run_epoch(session, train_model, train_batches,\n",
    "                                         train_model.train_op, True, step)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_pplx))\n",
    "\n",
    "            _, eval_pplx = run_epoch(session, eval_model, eval_batches,\n",
    "                                     tf.no_op(), False, 0)\n",
    "            print(\"Epoch: %d Eval Perplexity: %.3f\" % (i + 1, eval_pplx))\n",
    "\n",
    "        _, test_pplx = run_epoch(session, eval_model, test_batches,\n",
    "                                 tf.no_op(), False, 0)\n",
    "        print(\"Test Perplexity: %.3f\" % test_pplx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
